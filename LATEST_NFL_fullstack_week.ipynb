{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0e3ad0",
   "metadata": {},
   "source": [
    "# NFL Full‑Stack Predictions — Reproducible Weekly Run\n",
    "\n",
    "**Generated:** 2025-11-02T17:39:14\n",
    "\n",
    "Two spaces after periods.  Hyphens instead of em dashes.  This notebook is completely reproducible and self‑contained:\n",
    "- **Cell 1** fetches all needed data live, normalizes it into a **stable input schema**, and saves to a CSV.\n",
    "- **Cells 2‑N** run the full modeling stack (market prior, spread curve, injury and weather adjustments, Monte Carlo, Logistic Regression, Gradient‑Boosted Trees), then save and print the weekly summary.\n",
    "- All file names, seeds, and knobs are declared as **constants at the top**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5872ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONSTANTS — TUNE HERE\n",
    "# =========================\n",
    "SEASON = 2025\n",
    "WEEK   = None              # None = auto-detect from schedule.  Or set an int.\n",
    "INPUT_CSV  = \"data/nfl_week_inputs.csv\"\n",
    "OUTPUT_CSV = \"data/nfl_week_summary.csv\"\n",
    "SEED       = 20251102      # Reproducible seed for Monte Carlo\n",
    "SD_MARGIN  = 13.0          # League typical points variance for margin\n",
    "MC_SIMS    = 100_000       # Monte Carlo draws\n",
    "LOGIT_MAX_ITER = 400\n",
    "\n",
    "# Optional: path to real historical training set (if present, used instead of synthetic)\n",
    "NFLFASTR_GAMES = \"data/nflfastR_games.csv\"\n",
    "\n",
    "# Team code normalization map to 3-letter abbreviations used across sources\n",
    "TEAM_FIX = {\n",
    "    \"KAN\":\"KC\",\"JAC\":\"JAX\",\"GNB\":\"GB\",\"SFO\":\"SF\",\"NWE\":\"NE\",\"TAM\":\"TB\",\"NOR\":\"NO\",\"LVR\":\"LV\",\n",
    "    \"LAC\":\"LAC\",\"LAR\":\"LAR\",\"WSH\":\"WAS\",\"WFT\":\"WAS\",\"BLT\":\"BAL\",\"ARI\":\"ARI\",\"HST\":\"HOU\",\n",
    "    \"OAK\":\"LV\",\"SD\":\"LAC\",\"STL\":\"LAR\",\"LA\":\"LAR\",\"CLV\":\"CLE\",\"GBP\":\"GB\"\n",
    "}\n",
    "\n",
    "# Stable input schema the model expects.  Do not change ordering.\n",
    "SCHEMA = [\n",
    "    \"game_id\",\"kickoff_et\",\"home_team\",\"away_team\",\"favorite\",\"spread_fav\",\"total\",\n",
    "    \"ml_fav\",\"ml_dog\",\"home_city\",\"weather_precip\",\"weather_temp_f\",\"weather_breezy\",\n",
    "    \"inj_out_fav\",\"inj_doubt_fav\",\"inj_quest_fav\",\"inj_out_dog\",\"inj_doubt_dog\",\"inj_quest_dog\",\n",
    "    \"off_epa_fav\",\"def_epa_allowed_fav\",\"off_epa_dog\",\"def_epa_allowed_dog\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403b8d1",
   "metadata": {},
   "source": [
    "# 1) Pull live inputs and write a stable CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87a0b8d8-f503-446c-b767-e0d7daa4552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, io, requests, re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Helper: normalize team code to 3-letter\n",
    "def norm_team(x: str) -> str:\n",
    "    if pd.isna(x): \n",
    "        return x\n",
    "    x = str(x).strip().upper()\n",
    "    return TEAM_FIX.get(x, x[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f181365-dfd6-4da1-bc42-7d5e65d52c43",
   "metadata": {},
   "source": [
    "## 1a. Schedule — nfl_data_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19b321-26de-40f9-bcc3-27e7ba73f792",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### keep for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ce8db9e-483d-41f5-9b0f-11c1b83e9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No explicit start time column found in schedule.  Defaulting to 13:00 ET.\n"
     ]
    }
   ],
   "source": [
    "### KINDA WORKS - DOESNT SEE TIME\n",
    "# ============================================================\n",
    "# 1a. Schedule — nfl_data_py\n",
    "# ============================================================\n",
    "try:\n",
    "    from nfl_data_py import import_schedules\n",
    "    sched = import_schedules([SEASON])\n",
    "    # Normalize columns\n",
    "    if \"game_date\" in sched.columns:\n",
    "        sched[\"game_date\"] = pd.to_datetime(sched[\"game_date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        alt = [c for c in sched.columns if \"date\" in c.lower()]\n",
    "        if alt:\n",
    "            sched.rename(columns={alt[0]: \"game_date\"}, inplace=True)\n",
    "            sched[\"game_date\"] = pd.to_datetime(sched[\"game_date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "    # Auto-detect week if not provided\n",
    "    if WEEK is None:\n",
    "        today = pd.Timestamp.now(tz=\"America/New_York\").tz_localize(None).normalize()\n",
    "        sched[\"game_date\"] = pd.to_datetime(sched[\"game_date\"]).dt.tz_localize(None)\n",
    "        this_week = sched.loc[pd.to_datetime(sched[\"game_date\"]) <= today, \"week\"].max()\n",
    "        WEEK = int(this_week) if pd.notna(this_week) else int(sched[\"week\"].max())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Schedule fetch failed: {e}\")\n",
    "\n",
    "# Pick whichever columns nfl_data_py actually gives us\n",
    "home_col = next((c for c in sched.columns if \"home\" in c.lower() and \"team\" in c.lower()), None)\n",
    "away_col = next((c for c in sched.columns if \"away\" in c.lower() and \"team\" in c.lower()), None)\n",
    "if home_col is None or away_col is None:\n",
    "    raise RuntimeError(f\"Could not find home/away team columns. Found columns: {sched.columns.tolist()}\")\n",
    "\n",
    "# Pick a start-time column if it exists\n",
    "possible_time_cols = [\"start_time\", \"game_time_eastern\", \"game_time\", \"gamedetail\"]\n",
    "time_col = next((c for c in possible_time_cols if c in sched.columns), None)\n",
    "if time_col is None:\n",
    "    print(\"⚠️ No explicit start time column found in schedule.  Defaulting to 13:00 ET.\")\n",
    "    sched[\"start_time\"] = \"13:00\"\n",
    "else:\n",
    "    sched[\"start_time\"] = sched[time_col].fillna(\"13:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8c68d-e0c8-4f16-95fb-a680d6a0ce12",
   "metadata": {},
   "source": [
    "### newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a47c341-391e-473f-a5d8-f401c995bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schedule loaded successfully\n",
      "              game_id home_team away_team   game_date start_time\n",
      "7112  2025_09_BAL_MIA       MIA       BAL  2025-10-30      20:15\n",
      "7113  2025_09_CHI_CIN       CIN       CHI  2025-11-02      13:00\n",
      "7114  2025_09_MIN_DET       DET       MIN  2025-11-02      13:00\n",
      "7115   2025_09_CAR_GB        GB       CAR  2025-11-02      13:00\n",
      "7116  2025_09_DEN_HOU       HOU       DEN  2025-11-02      13:00\n",
      "7117   2025_09_ATL_NE        NE       ATL  2025-11-02      13:00\n",
      "7118   2025_09_SF_NYG       NYG        SF  2025-11-02      13:00\n",
      "7119  2025_09_IND_PIT       PIT       IND  2025-11-02      13:00\n",
      "7120  2025_09_LAC_TEN       TEN       LAC  2025-11-02      13:00\n",
      "7121    2025_09_NO_LA        LA        NO  2025-11-02      16:05\n",
      "7122   2025_09_JAX_LV        LV       JAX  2025-11-02      16:05\n",
      "7123   2025_09_KC_BUF       BUF        KC  2025-11-02      16:25\n",
      "7124  2025_09_SEA_WAS       WAS       SEA  2025-11-02      20:20\n",
      "7125  2025_09_ARI_DAL       DAL       ARI  2025-11-03      20:15\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1a. Schedule — nfl_data_py  (final, schema-safe)\n",
    "# ============================================================\n",
    "try:\n",
    "    from nfl_data_py import import_schedules\n",
    "\n",
    "    # Pull schedule\n",
    "    sched = import_schedules([SEASON]).copy()\n",
    "\n",
    "    # --- Ensure proper date field ---\n",
    "    if \"gameday\" in sched.columns:\n",
    "        sched.rename(columns={\"gameday\": \"game_date\"}, inplace=True)\n",
    "    elif \"__game_date__\" in sched.columns:\n",
    "        sched.rename(columns={\"__game_date__\": \"game_date\"}, inplace=True)\n",
    "    else:\n",
    "        raise RuntimeError(f\"No recognizable date field found in schedule: {sched.columns.tolist()}\")\n",
    "    sched[\"game_date\"] = pd.to_datetime(sched[\"game_date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # --- Ensure start time field ---\n",
    "    time_col = None\n",
    "    for c in [\"start_time\", \"__start_time__\", \"gametime\", \"game_time\"]:\n",
    "        if c in sched.columns:\n",
    "            time_col = c\n",
    "            break\n",
    "    if time_col is None:\n",
    "        print(\"⚠️ No explicit start time column found. Defaulting to 13:00 ET.\")\n",
    "        sched[\"start_time\"] = \"13:00\"\n",
    "    else:\n",
    "        # Normalize to string HH:MM\n",
    "        sched[\"start_time\"] = sched[time_col].astype(str).fillna(\"13:00\")\n",
    "\n",
    "    # --- Determine current or next week automatically ---\n",
    "    if WEEK is None:\n",
    "        today = pd.Timestamp.now(tz=\"America/New_York\").tz_localize(None).normalize()\n",
    "        sched[\"game_date\"] = pd.to_datetime(sched[\"game_date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "        this_week = sched.loc[pd.to_datetime(sched[\"game_date\"]) <= today, \"week\"].max()\n",
    "        WEEK = int(this_week) if pd.notna(this_week) else int(sched[\"week\"].max())\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Schedule fetch failed: {e}\")\n",
    "\n",
    "# --- Home/away fields (already standard in your data) ---\n",
    "home_col, away_col = \"home_team\", \"away_team\"\n",
    "\n",
    "# --- Guarantee the target columns exist before slicing ---\n",
    "required_cols = [\"game_id\", home_col, away_col, \"game_date\", \"start_time\"]\n",
    "missing = [c for c in required_cols if c not in sched.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing expected columns in sched: {missing}\")\n",
    "\n",
    "# --- Build trimmed schedule table ---\n",
    "games = sched.loc[sched[\"season\"].eq(SEASON) & sched[\"week\"].eq(WEEK), required_cols].copy()\n",
    "\n",
    "print(\"✅ Schedule loaded successfully\")\n",
    "print(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bbb6c94-0e04-4fdc-875e-9bd72d0ec89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build a trimmed schedule table with canonical names (final fix) ---\n",
    "# Look for any column that represents the game date or gameday\n",
    "date_col_candidates = [c for c in sched.columns if any(k in c.lower() for k in [\"gameday\", \"game_date\", \"date\", \"gametime\"])]\n",
    "if not date_col_candidates:\n",
    "    raise RuntimeError(f\"No usable date column found in schedule: {sched.columns.tolist()}\")\n",
    "date_col = date_col_candidates[0]\n",
    "\n",
    "# If it's named 'gameday' or 'gametime', normalize its contents to a proper date string\n",
    "sched[date_col] = pd.to_datetime(sched[date_col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Build consistent columns (guaranteed to exist now)\n",
    "expected_cols = [\"game_id\", home_col, away_col, date_col, \"start_time\"]\n",
    "for col in expected_cols:\n",
    "    if col not in sched.columns:\n",
    "        sched[col] = np.nan\n",
    "\n",
    "games = (\n",
    "    sched.query(\"season == @SEASON and week == @WEEK\")[expected_cols]\n",
    "         .rename(columns={\n",
    "             home_col: \"home_team\",\n",
    "             away_col: \"away_team\",\n",
    "             date_col: \"game_date\"\n",
    "         })\n",
    "         .copy()\n",
    ")\n",
    "\n",
    "# Normalize team codes\n",
    "games[\"home_team\"] = games[\"home_team\"].map(norm_team)\n",
    "games[\"away_team\"] = games[\"away_team\"].map(norm_team)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bb84d-f504-41a2-bee2-c0438680471e",
   "metadata": {},
   "source": [
    "#### print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b8c6ef-27c9-442b-a2a2-d0efa097dd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game_id', 'season', 'game_type', 'week', 'game_date', 'weekday', 'gametime', 'away_team', 'away_score', 'home_team', 'home_score', 'location', 'result', 'total', 'overtime', 'old_game_id', 'gsis', 'nfl_detail_id', 'pfr', 'pff', 'espn', 'ftn', 'away_rest', 'home_rest', 'away_moneyline', 'home_moneyline', 'spread_line', 'away_spread_odds', 'home_spread_odds', 'total_line', 'under_odds', 'over_odds', 'div_game', 'roof', 'surface', 'temp', 'wind', 'away_qb_id', 'home_qb_id', 'away_qb_name', 'home_qb_name', 'away_coach', 'home_coach', 'referee', 'stadium_id', 'stadium', 'start_time']\n",
      "              game_id  season game_type  week   game_date   weekday gametime  \\\n",
      "6991  2025_01_DAL_PHI    2025       REG     1  2025-09-04  Thursday    20:20   \n",
      "6992   2025_01_KC_LAC    2025       REG     1  2025-09-05    Friday    20:00   \n",
      "6993   2025_01_TB_ATL    2025       REG     1  2025-09-07    Sunday    13:00   \n",
      "\n",
      "     away_team  away_score home_team  ...  away_qb_id  home_qb_id  \\\n",
      "6991       DAL        20.0       PHI  ...  00-0033077  00-0036389   \n",
      "6992        KC        21.0       LAC  ...  00-0033873  00-0036355   \n",
      "6993        TB        23.0       ATL  ...  00-0034855  00-0039917   \n",
      "\n",
      "         away_qb_name    home_qb_name            away_coach     home_coach  \\\n",
      "6991     Dak Prescott     Jalen Hurts  Brian Schottenheimer  Nick Sirianni   \n",
      "6992  Patrick Mahomes  Justin Herbert             Andy Reid   Jim Harbaugh   \n",
      "6993   Baker Mayfield   Michael Penix           Todd Bowles  Raheem Morris   \n",
      "\n",
      "            referee stadium_id                  stadium  start_time  \n",
      "6991    Shawn Smith      PHI00  Lincoln Financial Field       20:20  \n",
      "6992  Carl Cheffers      LAX01             SoFi Stadium       20:00  \n",
      "6993     Land Clark      ATL97    Mercedes-Benz Stadium       13:00  \n",
      "\n",
      "[3 rows x 47 columns]\n",
      "['game_id', 'home_team', 'away_team', 'game_date', 'start_time']\n",
      "              game_id home_team away_team   game_date start_time\n",
      "7112  2025_09_BAL_MIA       MIA       BAL  2025-10-30      20:15\n",
      "7113  2025_09_CHI_CIN       CIN       CHI  2025-11-02      13:00\n",
      "7114  2025_09_MIN_DET       DET       MIN  2025-11-02      13:00\n"
     ]
    }
   ],
   "source": [
    "print(sched.columns.tolist())\n",
    "print(sched.head(3))\n",
    "\n",
    "print(games.columns.tolist())\n",
    "print(games.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9267922-c025-44c9-867c-37a03c199f12",
   "metadata": {},
   "source": [
    "## 1b. Odds — ESPN scoreboard JSON (public)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f44fd-2e96-47d0-bf1f-320750dcf803",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### pull using espn (future/live only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a0c85-cf1a-44a5-bd4d-6f33021890b9",
   "metadata": {},
   "source": [
    "#### endpoint was working, but code wasn't parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4c510c6-7603-4fc1-8a13-76aa8502d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1b. Odds — ESPN scoreboard JSON (public)\n",
    "# ============================================================\n",
    "def fetch_odds_espn():\n",
    "    url = \"https://site.web.api.espn.com/apis/v2/scoreboard/header?sport=football&league=nfl\"\n",
    "    r = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    j = r.json()\n",
    "    rows = []\n",
    "    for ev in j.get(\"events\", []):\n",
    "        comps = ev.get(\"competitions\", [])\n",
    "        if not comps: \n",
    "            continue\n",
    "        c = comps[0]\n",
    "        if \"competitors\" not in c or len(c[\"competitors\"]) < 2: \n",
    "            continue\n",
    "        h = c[\"competitors\"][0]; a = c[\"competitors\"][1]\n",
    "        home = norm_team(h[\"team\"][\"abbreviation\"])\n",
    "        away = norm_team(a[\"team\"][\"abbreviation\"])\n",
    "        spread_fav = None; total = None; ml_fav = None\n",
    "        if c.get(\"odds\"):\n",
    "            o = c[\"odds\"][0]\n",
    "            total = o.get(\"overUnder\", np.nan)\n",
    "            try:\n",
    "                ml_fav = float(o.get(\"moneyLine\")) if o.get(\"moneyLine\") is not None else np.nan\n",
    "            except: \n",
    "                ml_fav = np.nan\n",
    "            det = str(o.get(\"details\", \"\"))\n",
    "            m = re.search(r\"([A-Z]{2,3})\\s*[-−](\\d+\\.?\\d*)\", det)\n",
    "            if m:\n",
    "                fav_abbr = norm_team(m.group(1))\n",
    "                val = float(m.group(2))\n",
    "                spread_fav = val\n",
    "        rows.append({\n",
    "            \"home_team\": home, \n",
    "            \"away_team\": away,\n",
    "            \"spread_fav\": spread_fav, \n",
    "            \"total\": total, \n",
    "            \"ml_fav\": ml_fav\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "odds = fetch_odds_espn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80026c87-6bf1-48f6-af35-a359da57a8a0",
   "metadata": {},
   "source": [
    "#### end point working, code parsing, no history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f7a51aa-9549-4413-8b34-9b3ce7ae4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pulled 14 games from ESPN odds feed\n",
      "  home_team away_team  spread_fav  total  ml_fav\n",
      "0       CIN       CHI         NaN    NaN     NaN\n",
      "1       DET       MIN         NaN    NaN     NaN\n",
      "2        GB       CAR         NaN    NaN     NaN\n",
      "3       TEN       LAC         NaN    NaN     NaN\n",
      "4        NE       ATL         NaN    NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1b. Odds — ESPN main scoreboard JSON (working endpoint)\n",
    "# ============================================================\n",
    "def fetch_odds_espn():\n",
    "    \"\"\"\n",
    "    Fetches NFL odds and totals from ESPN's live scoreboard API.\n",
    "    Returns DataFrame with columns: home_team, away_team, spread_fav, total, ml_fav\n",
    "    \"\"\"\n",
    "    url = \"https://site.api.espn.com/apis/site/v2/sports/football/nfl/scoreboard\"\n",
    "    r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "\n",
    "    rows = []\n",
    "    for ev in j.get(\"events\", []):\n",
    "        comps = ev.get(\"competitions\", [])\n",
    "        if not comps:\n",
    "            continue\n",
    "        c = comps[0]\n",
    "        if \"competitors\" not in c or len(c[\"competitors\"]) < 2:\n",
    "            continue\n",
    "\n",
    "        # Competitor roles are marked \"home\"/\"away\" in this endpoint\n",
    "        home, away = None, None\n",
    "        for team in c[\"competitors\"]:\n",
    "            abbr = norm_team(team[\"team\"][\"abbreviation\"])\n",
    "            if team[\"homeAway\"] == \"home\":\n",
    "                home = abbr\n",
    "            else:\n",
    "                away = abbr\n",
    "\n",
    "        spread_fav, total, ml_fav = np.nan, np.nan, np.nan\n",
    "        if \"odds\" in c and c[\"odds\"]:\n",
    "            o = c[\"odds\"][0]\n",
    "            total = o.get(\"overUnder\", np.nan)\n",
    "            try:\n",
    "                ml_fav = float(o.get(\"moneyLine\")) if o.get(\"moneyLine\") is not None else np.nan\n",
    "            except:\n",
    "                ml_fav = np.nan\n",
    "\n",
    "            # Parse favorite and spread\n",
    "            det = str(o.get(\"details\", \"\"))\n",
    "            m = re.search(r\"([A-Z]{2,3})\\s*[-−](\\d+\\.?\\d*)\", det)\n",
    "            if m:\n",
    "                fav_abbr = norm_team(m.group(1))\n",
    "                spread_fav = float(m.group(2))\n",
    "            else:\n",
    "                spread_fav = np.nan\n",
    "\n",
    "        if home and away:\n",
    "            rows.append({\n",
    "                \"home_team\": home,\n",
    "                \"away_team\": away,\n",
    "                \"spread_fav\": spread_fav,\n",
    "                \"total\": total,\n",
    "                \"ml_fav\": ml_fav\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"✅ Pulled {len(df)} games from ESPN odds feed\")\n",
    "    return df\n",
    "\n",
    "odds = fetch_odds_espn()\n",
    "print(odds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beafadf-3f74-4eb7-8f53-812ae4d2defd",
   "metadata": {},
   "source": [
    "### pull using odds-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ae17ae1-d3ac-4a9e-b568-67fd587dd670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              home_team             away_team  spread  total bookmaker  \\\n",
      "0  New England Patriots       Atlanta Falcons    17.5   51.0    Bovada   \n",
      "1   Pittsburgh Steelers    Indianapolis Colts     3.0   51.5    Bovada   \n",
      "2    Cincinnati Bengals         Chicago Bears    -6.5   60.5    Bovada   \n",
      "3      Tennessee Titans  Los Angeles Chargers    -7.5   61.5    Bovada   \n",
      "4     Green Bay Packers     Carolina Panthers     6.0   35.5    Bovada   \n",
      "\n",
      "          commence_time  \n",
      "0  2025-11-02T18:02:21Z  \n",
      "1  2025-11-02T18:02:29Z  \n",
      "2  2025-11-02T18:02:36Z  \n",
      "3  2025-11-02T18:02:45Z  \n",
      "4  2025-11-02T18:02:52Z  \n"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd\n",
    "\n",
    "API_KEY = \"a1b9bc38f3e9c4d2e8a6ce056ec078e6\"\n",
    "url = f\"https://api.the-odds-api.com/v4/sports/americanfootball_nfl/odds/?apiKey={API_KEY}&regions=us&markets=spreads,totals,h2h\"\n",
    "\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "\n",
    "rows = []\n",
    "for g in data:\n",
    "    home = g[\"home_team\"]\n",
    "    away = g[\"away_team\"]\n",
    "    for bk in g[\"bookmakers\"]:\n",
    "        for mk in bk[\"markets\"]:\n",
    "            if mk[\"key\"] == \"spreads\":\n",
    "                spread = mk[\"outcomes\"][0][\"point\"]\n",
    "            elif mk[\"key\"] == \"totals\":\n",
    "                total = mk[\"outcomes\"][0][\"point\"]\n",
    "    rows.append({\n",
    "        \"home_team\": home,\n",
    "        \"away_team\": away,\n",
    "        \"spread\": spread,\n",
    "        \"total\": total,\n",
    "        \"bookmaker\": bk[\"title\"],\n",
    "        \"commence_time\": g[\"commence_time\"]\n",
    "    })\n",
    "odds = pd.DataFrame(rows)\n",
    "print(odds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "407dca9a-0220-45d2-9928-0d19c51e3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example normalization (extend as needed)\n",
    "TEAM_ABBR = {\n",
    "    \"Arizona Cardinals\": \"ARI\", \"Atlanta Falcons\": \"ATL\", \"Baltimore Ravens\": \"BAL\",\n",
    "    \"Buffalo Bills\": \"BUF\", \"Carolina Panthers\": \"CAR\", \"Chicago Bears\": \"CHI\",\n",
    "    \"Cincinnati Bengals\": \"CIN\", \"Cleveland Browns\": \"CLE\", \"Dallas Cowboys\": \"DAL\",\n",
    "    \"Denver Broncos\": \"DEN\", \"Detroit Lions\": \"DET\", \"Green Bay Packers\": \"GB\",\n",
    "    \"Houston Texans\": \"HOU\", \"Indianapolis Colts\": \"IND\", \"Jacksonville Jaguars\": \"JAX\",\n",
    "    \"Kansas City Chiefs\": \"KC\", \"Las Vegas Raiders\": \"LV\", \"Los Angeles Chargers\": \"LAC\",\n",
    "    \"Los Angeles Rams\": \"LAR\", \"Miami Dolphins\": \"MIA\", \"Minnesota Vikings\": \"MIN\",\n",
    "    \"New England Patriots\": \"NE\", \"New Orleans Saints\": \"NO\", \"New York Giants\": \"NYG\",\n",
    "    \"New York Jets\": \"NYJ\", \"Philadelphia Eagles\": \"PHI\", \"Pittsburgh Steelers\": \"PIT\",\n",
    "    \"San Francisco 49ers\": \"SF\", \"Seattle Seahawks\": \"SEA\", \"Tampa Bay Buccaneers\": \"TB\",\n",
    "    \"Tennessee Titans\": \"TEN\", \"Washington Commanders\": \"WAS\"\n",
    "}\n",
    "\n",
    "odds[\"home_team\"] = odds[\"home_team\"].map(TEAM_ABBR)\n",
    "odds[\"away_team\"] = odds[\"away_team\"].map(TEAM_ABBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "659a7374-23fb-447f-ac1c-556bd5c230c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            game_id home_team away_team  spread  total\n",
      "0   2025_09_BAL_MIA       MIA       BAL     NaN    NaN\n",
      "1   2025_09_CHI_CIN       CIN       CHI    -6.5   60.5\n",
      "2   2025_09_MIN_DET       DET       MIN    -2.0   58.5\n",
      "3    2025_09_CAR_GB        GB       CAR     6.0   35.5\n",
      "4   2025_09_DEN_HOU       HOU       DEN    -1.5   36.5\n",
      "5    2025_09_ATL_NE        NE       ATL    17.5   51.0\n",
      "6    2025_09_SF_NYG       NYG        SF     7.0   51.0\n",
      "7   2025_09_IND_PIT       PIT       IND     3.0   51.5\n",
      "8   2025_09_LAC_TEN       TEN       LAC    -7.5   61.5\n",
      "9     2025_09_NO_LA        LA        NO     NaN    NaN\n",
      "10   2025_09_JAX_LV        LV       JAX    -1.5   44.5\n",
      "11   2025_09_KC_BUF       BUF        KC     2.0   53.0\n",
      "12  2025_09_SEA_WAS       WAS       SEA    -3.0   48.0\n",
      "13  2025_09_ARI_DAL       DAL       ARI     3.5   53.5\n"
     ]
    }
   ],
   "source": [
    "merged = games.merge(odds, on=[\"home_team\", \"away_team\"], how=\"left\")\n",
    "print(merged[[\"game_id\",\"home_team\",\"away_team\",\"spread\",\"total\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed397a-5c33-4fcc-8408-8ba519aa5322",
   "metadata": {},
   "source": [
    "### print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8268f7e3-c45e-4688-bdfc-6a56d97c66c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['home_team', 'away_team', 'spread', 'total', 'bookmaker', 'commence_time']\n",
      "   home_team away_team  spread  total bookmaker         commence_time\n",
      "0         NE       ATL    17.5   51.0    Bovada  2025-11-02T18:02:21Z\n",
      "1        PIT       IND     3.0   51.5    Bovada  2025-11-02T18:02:29Z\n",
      "2        CIN       CHI    -6.5   60.5    Bovada  2025-11-02T18:02:36Z\n",
      "3        TEN       LAC    -7.5   61.5    Bovada  2025-11-02T18:02:45Z\n",
      "4         GB       CAR     6.0   35.5    Bovada  2025-11-02T18:02:52Z\n",
      "5        NYG        SF     7.0   51.0    Bovada  2025-11-02T18:02:52Z\n",
      "6        HOU       DEN    -1.5   36.5    Bovada  2025-11-02T18:02:56Z\n",
      "7        DET       MIN    -2.0   58.5    Bovada  2025-11-02T18:03:03Z\n",
      "8         LV       JAX    -1.5   44.5    Bovada  2025-11-02T21:05:00Z\n",
      "9        LAR        NO   -14.5   44.5    Bovada  2025-11-02T21:05:00Z\n",
      "10       BUF        KC     2.0   53.0    Bovada  2025-11-02T21:25:00Z\n",
      "11       WAS       SEA    -3.0   48.0    Bovada  2025-11-03T01:20:00Z\n",
      "12       DAL       ARI     3.5   53.5  Fanatics  2025-11-04T01:15:00Z\n"
     ]
    }
   ],
   "source": [
    "print(odds.columns.tolist())\n",
    "print(odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1bad76",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'home_team'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m inj_away \u001b[38;5;241m=\u001b[39m inj\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteam\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maway_team\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_out\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_out_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_doubt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_doubt_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_quest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_quest_away\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m }) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inj\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maway_team\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_out_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_doubt_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_quest_away\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# --- Merge everything safely ---\u001b[39;00m\n\u001b[1;32m     43\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mgames\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepa_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhome_team\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepa_away\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maway_team\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43minj_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhome_team\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43minj_away\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maway_team\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43modds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhome_team\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maway_team\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# --- Fill numerics and compute features ---\u001b[39;00m\n\u001b[1;32m     53\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff_epa_home\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef_epa_allowed_home\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff_epa_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef_epa_allowed_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_out_home\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_doubt_home\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_quest_home\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_out_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_doubt_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minj_quest_away\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspread_fav\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml_fav\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:10093\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10074\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10075\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10089\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10091\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10094\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10103\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:703\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross \u001b[38;5;241m=\u001b[39m cross_col\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# note this function has side effects\u001b[39;00m\n\u001b[1;32m    699\u001b[0m (\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[0;32m--> 703\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1162\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m rk \u001b[38;5;241m=\u001b[39m cast(Hashable, rk)\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     right_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     right_keys\u001b[38;5;241m.\u001b[39mappend(right\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/generic.py:1850\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     values \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\n\u001b[1;32m   1846\u001b[0m         \u001b[38;5;241m.\u001b[39mget_level_values(key)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m         \u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'home_team'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1c. Weather — Open-Meteo geocoder + forecast\n",
    "# ============================================================\n",
    "CITY_BY_TEAM = {\n",
    "    \"ARI\":\"Glendale,US\",\"ATL\":\"Atlanta,US\",\"BAL\":\"Baltimore,US\",\"BUF\":\"Orchard Park,US\",\n",
    "    \"CAR\":\"Charlotte,US\",\"CHI\":\"Chicago,US\",\"CIN\":\"Cincinnati,US\",\"CLE\":\"Cleveland,US\",\n",
    "    \"DAL\":\"Arlington,US\",\"DEN\":\"Denver,US\",\"DET\":\"Detroit,US\",\"GB\":\"Green Bay,US\",\n",
    "    \"HOU\":\"Houston,US\",\"IND\":\"Indianapolis,US\",\"JAX\":\"Jacksonville,US\",\"KC\":\"Kansas City,US\",\n",
    "    \"LAC\":\"Inglewood,US\",\"LAR\":\"Inglewood,US\",\"LV\":\"Las Vegas,US\",\"MIA\":\"Miami Gardens,US\",\n",
    "    \"MIN\":\"Minneapolis,US\",\"NE\":\"Foxborough,US\",\"NO\":\"New Orleans,US\",\"NYG\":\"East Rutherford,US\",\n",
    "    \"NYJ\":\"East Rutherford,US\",\"PHI\":\"Philadelphia,US\",\"PIT\":\"Pittsburgh,US\",\"SEA\":\"Seattle,US\",\n",
    "    \"SF\":\"Santa Clara,US\",\"TB\":\"Tampa,US\",\"TEN\":\"Nashville,US\",\"WAS\":\"Landover,US\"\n",
    "}\n",
    "\n",
    "def fetch_weather(team):\n",
    "    city = CITY_BY_TEAM.get(team)\n",
    "    if not city: \n",
    "        return (np.nan, np.nan, np.nan, \"\")\n",
    "    try:\n",
    "        geo = requests.get(\n",
    "            f\"https://geocoding-api.open-meteo.com/v1/search?name={city.replace(' ','+')}\",\n",
    "            headers={\"User-Agent\":\"Mozilla/5.0\"}).json()\n",
    "        lat, lon = geo[\"results\"][0][\"latitude\"], geo[\"results\"][0][\"longitude\"]\n",
    "        w = requests.get(\n",
    "            f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&hourly=temperature_2m,precipitation_probability,windspeed_10m&forecast_days=1\",\n",
    "            headers={\"User-Agent\":\"Mozilla/5.0\"}).json()\n",
    "        t = float(np.nanmean(w[\"hourly\"][\"temperature_2m\"]))\n",
    "        p = float(np.nanmean(w[\"hourly\"][\"precipitation_probability\"]))\n",
    "        wind = float(np.nanmean(w[\"hourly\"][\"windspeed_10m\"]))\n",
    "        breezy = 1 if wind >= 12 else 0\n",
    "        return (p, t, breezy, city)\n",
    "    except Exception:\n",
    "        return (np.nan, np.nan, np.nan, city or \"\")\n",
    "\n",
    "wx_rows = [fetch_weather(t) for t in games[\"home_team\"]]\n",
    "games[[\"weather_precip\",\"weather_temp_f\",\"weather_breezy\",\"home_city\"]] = pd.DataFrame(wx_rows, index=games.index)\n",
    "\n",
    "# ============================================================\n",
    "# 1d. EPA/play — compute from play-by-play\n",
    "# ============================================================\n",
    "try:\n",
    "    from nfl_data_py import import_pbp_data as import_pbp\n",
    "except ImportError:\n",
    "    from nfl_data_py import import_pbp as import_pbp\n",
    "\n",
    "def compute_team_epa_from_pbp(season: int, weeks=None):\n",
    "    pbp = import_pbp([season])\n",
    "    if weeks is not None:\n",
    "        pbp = pbp[pbp[\"week\"].isin(weeks)]\n",
    "    mask = (\n",
    "        pbp[\"play_type\"].isin([\"pass\",\"run\"])\n",
    "        & (pbp[\"qb_spike\"].fillna(0)==0)\n",
    "        & (pbp[\"qb_kneel\"].fillna(0)==0)\n",
    "        & (pbp[\"penalty\"].fillna(0)==0)\n",
    "    )\n",
    "    pbp = pbp.loc[mask, [\"posteam\",\"defteam\",\"epa\"]]\n",
    "    off = pbp.groupby(\"posteam\")[\"epa\"].mean().reset_index().rename(columns={\"posteam\":\"team\",\"epa\":\"off_epa\"})\n",
    "    deff = pbp.groupby(\"defteam\")[\"epa\"].mean().reset_index().rename(columns={\"defteam\":\"team\",\"epa\":\"def_epa_allowed\"})\n",
    "    epa = off.merge(deff, on=\"team\", how=\"outer\")\n",
    "    epa[\"team\"] = epa[\"team\"].astype(str).str.upper().str.replace(r\"\\W+\",\"\",regex=True).str[:3]\n",
    "    epa[\"team\"] = epa[\"team\"].map(lambda x: TEAM_FIX.get(x, x))\n",
    "    return epa\n",
    "\n",
    "epa = compute_team_epa_from_pbp(SEASON)\n",
    "epa_home = epa.add_suffix(\"_home\").rename(columns={\"team_home\":\"team_h\"})\n",
    "epa_away = epa.add_suffix(\"_away\").rename(columns={\"team_away\":\"team_a\"})\n",
    "\n",
    "# ============================================================\n",
    "# 1e. Injuries — FantasyPros scrape\n",
    "# ============================================================\n",
    "def fetch_injuries():\n",
    "    url = \"https://www.fantasypros.com/nfl/injuries/\"\n",
    "    r = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    rows = []\n",
    "    for tr in soup.select(\"tbody tr\"):\n",
    "        tds = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        if len(tds) < 3: \n",
    "            continue\n",
    "        team = norm_team(tds[1])\n",
    "        status = tds[-1].lower()\n",
    "        rows.append({\"team\": team, \"status\": status})\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"team\",\"inj_out\",\"inj_doubt\",\"inj_quest\"])\n",
    "    df = pd.DataFrame(rows)\n",
    "    agg = df.groupby([\"team\",\"status\"]).size().unstack(fill_value=0)\n",
    "    for s in [\"out\",\"doubtful\",\"questionable\"]:\n",
    "        if s not in agg.columns:\n",
    "            agg[s] = 0\n",
    "    agg = agg.reset_index().rename(columns={\"out\":\"inj_out\",\"doubtful\":\"inj_doubt\",\"questionable\":\"inj_quest\"})\n",
    "    return agg\n",
    "\n",
    "inj = fetch_injuries()\n",
    "inj_home = inj.add_suffix(\"_home\").rename(columns={\"team_home\":\"team_h\"})\n",
    "inj_away = inj.add_suffix(\"_away\").rename(columns={\"team_away\":\"team_a\"})\n",
    "\n",
    "# ============================================================\n",
    "# 1f. Merge everything cleanly\n",
    "# ============================================================\n",
    "df = (\n",
    "    games.merge(epa_home, left_on=\"home_team\", right_on=\"team_h\", how=\"left\")\n",
    "         .merge(epa_away, left_on=\"away_team\", right_on=\"team_a\", how=\"left\")\n",
    "         .merge(inj_home, left_on=\"home_team\", right_on=\"team_h\", how=\"left\")\n",
    "         .merge(inj_away, left_on=\"away_team\", right_on=\"team_a\", how=\"left\")\n",
    "         .merge(odds, on=[\"home_team\",\"away_team\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Fill missing numeric values\n",
    "for col in [\"inj_out_home\",\"inj_doubt_home\",\"inj_quest_home\",\"inj_out_away\",\"inj_doubt_away\",\"inj_quest_away\",\n",
    "            \"off_epa_home\",\"def_epa_allowed_home\",\"off_epa_away\",\"def_epa_allowed_away\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "\n",
    "df[\"kickoff_et\"] = df[\"game_date\"].astype(str) + \" \" + df[\"start_time\"].fillna(\"13:00\")\n",
    "df[\"favorite\"] = np.where(df[\"spread_fav\"].notna(),\n",
    "                          df.apply(lambda r: r[\"away_team\"] if float(r[\"spread_fav\"])>0 else r[\"home_team\"], axis=1),\n",
    "                          df[\"home_team\"])\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"game_id\": df[\"game_id\"],\n",
    "    \"kickoff_et\": df[\"kickoff_et\"],\n",
    "    \"home_team\": df[\"home_team\"],\n",
    "    \"away_team\": df[\"away_team\"],\n",
    "    \"favorite\": df[\"favorite\"],\n",
    "    \"spread_fav\": pd.to_numeric(df[\"spread_fav\"], errors=\"coerce\"),\n",
    "    \"total\": pd.to_numeric(df[\"total\"], errors=\"coerce\"),\n",
    "    \"ml_fav\": pd.to_numeric(df[\"ml_fav\"], errors=\"coerce\"),\n",
    "    \"ml_dog\": np.nan,\n",
    "    \"home_city\": df[\"home_city\"],\n",
    "    \"weather_precip\": pd.to_numeric(df[\"weather_precip\"], errors=\"coerce\"),\n",
    "    \"weather_temp_f\": pd.to_numeric(df[\"weather_temp_f\"], errors=\"coerce\"),\n",
    "    \"weather_breezy\": pd.to_numeric(df[\"weather_breezy\"], errors=\"coerce\"),\n",
    "    \"inj_out_fav\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"inj_out_home\"], df[\"inj_out_away\"]),\n",
    "    \"inj_doubt_fav\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"inj_doubt_home\"], df[\"inj_doubt_away\"]),\n",
    "    \"inj_quest_fav\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"inj_quest_home\"], df[\"inj_quest_away\"]),\n",
    "    \"inj_out_dog\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"inj_out_away\"], df[\"inj_out_home\"]),\n",
    "    \"inj_doubt_dog\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"inj_doubt_away\"], df[\"inj_doubt_home\"]),\n",
    "    \"inj_quest_dog\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"inj_quest_away\"], df[\"inj_quest_home\"]),\n",
    "    \"off_epa_fav\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"off_epa_home\"], df[\"off_epa_away\"]),\n",
    "    \"def_epa_allowed_fav\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"def_epa_allowed_home\"], df[\"def_epa_allowed_away\"]),\n",
    "    \"off_epa_dog\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"off_epa_away\"], df[\"off_epa_home\"]),\n",
    "    \"def_epa_allowed_dog\": np.where(df[\"favorite\"] == df[\"home_team\"], df[\"def_epa_allowed_away\"], df[\"def_epa_allowed_home\"]),\n",
    "})\n",
    "\n",
    "out = out[SCHEMA].copy()\n",
    "out.to_csv(INPUT_CSV, index=False)\n",
    "print(f\"✅ Wrote stable weekly input file to {INPUT_CSV}\")\n",
    "print(out.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ad4438-cd95-4000-88d2-f808e9301bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team', 'off_epa', 'def_epa_allowed']\n",
      "  team   off_epa  def_epa_allowed\n",
      "0  ARI  0.003159         0.021111\n",
      "1  ATL -0.038489         0.001264\n",
      "2  BAL  0.027466         0.111421\n"
     ]
    }
   ],
   "source": [
    "print(epa.columns.tolist())\n",
    "print(epa.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcb472",
   "metadata": {},
   "source": [
    "## 2) Modeling utilities and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, math, pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pathlib import Path\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def implied_prob(odds):\n",
    "    if pd.isna(odds): return np.nan\n",
    "    return (-odds)/((-odds)+100) if odds < 0 else 100/(odds+100)\n",
    "\n",
    "def fair_probs(ml_fav, ml_dog):\n",
    "    if pd.isna(ml_fav) or pd.isna(ml_dog):\n",
    "        return np.nan, np.nan, np.nan\n",
    "    p_f = implied_prob(ml_fav); p_d = implied_prob(ml_dog)\n",
    "    vig = p_f + p_d - 1.0\n",
    "    return p_f/(1+vig), p_d/(1+vig), vig\n",
    "\n",
    "def spread_to_prob(spread, k=0.18):\n",
    "    if pd.isna(spread): return np.nan\n",
    "    return 1/(1 + math.exp(-k*spread))\n",
    "\n",
    "def injury_penalty(o, d, q):\n",
    "    o = 0 if pd.isna(o) else o\n",
    "    d = 0 if pd.isna(d) else d\n",
    "    q = 0 if pd.isna(q) else q\n",
    "    return 0.010*o + 0.006*d + 0.002*q\n",
    "\n",
    "def monte_carlo_cover(spread, mu_margin, sd=SD_MARGIN, N=MC_SIMS, seed=SEED):\n",
    "    r = np.random.default_rng(seed)\n",
    "    margins = r.normal(loc=mu_margin, scale=sd, size=N)\n",
    "    return float(np.mean(margins > spread)), float(np.mean(margins > 0)), np.percentile(margins,[5,25,50,75,95]).tolist()\n",
    "\n",
    "def train_models_synth(n=20000, sd=SD_MARGIN, seed=SEED):\n",
    "    r = np.random.default_rng(seed)\n",
    "    spread = r.uniform(0, 12, size=n)\n",
    "    off_diff_fav = r.normal(0, 0.10, size=n)\n",
    "    off_diff_dog = r.normal(0, 0.10, size=n)\n",
    "    mu = 0.75*spread + 25*off_diff_fav - 20*off_diff_dog\n",
    "    margin = r.normal(loc=mu, scale=sd, size=n)\n",
    "    covered = (margin > spread).astype(int)\n",
    "    X = np.column_stack([spread, off_diff_fav, off_diff_dog]); y = covered\n",
    "    logit = LogisticRegression(max_iter=LOGIT_MAX_ITER).fit(X,y)\n",
    "    gb = GradientBoostingClassifier(random_state=seed).fit(X,y)\n",
    "    return logit, gb, roc_auc_score(y, logit.predict_proba(X)[:,1]), roc_auc_score(y, gb.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24fddc",
   "metadata": {},
   "source": [
    "## 3) Optional real‑data training (if file present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a753bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_real = gb_real = None\n",
    "auc_logit_real = auc_gb_real = None\n",
    "\n",
    "p = Path(NFLFASTR_GAMES)\n",
    "if p.exists():\n",
    "    try:\n",
    "        real = pd.read_csv(p)\n",
    "        need = {'spread_line','off_epa_fav','def_epa_fav','off_epa_dog','def_epa_dog','covered'}\n",
    "        if need.issubset(real.columns):\n",
    "            real = real.dropna(subset=list(need)).copy()\n",
    "            real['off_diff_fav'] = real['off_epa_fav'] - real['def_epa_dog']\n",
    "            real['off_diff_dog'] = real['off_epa_dog'] - real['def_epa_fav']\n",
    "            Xr = real[['spread_line','off_diff_fav','off_diff_dog']].values\n",
    "            yr = real['covered'].values.astype(int)\n",
    "            logit_real = LogisticRegression(max_iter=LOGIT_MAX_ITER).fit(Xr, yr)\n",
    "            gb_real    = GradientBoostingClassifier(random_state=SEED).fit(Xr, yr)\n",
    "            auc_logit_real = roc_auc_score(yr, logit_real.predict_proba(Xr)[:,1])\n",
    "            auc_gb_real    = roc_auc_score(yr, gb_real.predict_proba(Xr)[:,1])\n",
    "            print(f\"Trained real‑data models.  AUC logit={auc_logit_real:.3f}, GBM={auc_gb_real:.3f}\")\n",
    "        else:\n",
    "            print(\"Real training file present but missing needed columns.  Using synthetic models.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Real‑data training failed: {e}.  Using synthetic models.\")\n",
    "else:\n",
    "    print(\"No nflfastR training file found.  Using synthetic models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df083190",
   "metadata": {},
   "source": [
    "## 4) Train synthetic cover models (fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_s, gb_s, aucL_s, aucG_s = train_models_synth()\n",
    "print(f\"Synthetic models: AUC logit={aucL_s:.3f}, GBM={aucG_s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d7bd9",
   "metadata": {},
   "source": [
    "## 5) Evaluate the weekly slate and build summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ac303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "inp = pd.read_csv(INPUT_CSV)\n",
    "rows = []\n",
    "\n",
    "for _, g in inp.iterrows():\n",
    "    fav = g[\"favorite\"]; dog = g[\"home_team\"] if g[\"away_team\"]==fav else g[\"away_team\"]\n",
    "    home_is_fav = (g[\"home_team\"]==fav)\n",
    "\n",
    "    # Prior probabilities\n",
    "    # If ml_dog missing, we blend spread mapping with any ML fav we have\n",
    "    p_fair_fav, p_fair_dog, vig = fair_probs(g[\"ml_fav\"], g[\"ml_dog\"])\n",
    "    p_spread = spread_to_prob(g[\"spread_fav\"])\n",
    "\n",
    "    pen_fav = injury_penalty(g[\"inj_out_fav\"], g[\"inj_doubt_fav\"], g[\"inj_quest_fav\"])\n",
    "    pen_dog = injury_penalty(g[\"inj_out_dog\"], g[\"inj_doubt_dog\"], g[\"inj_quest_dog\"])\n",
    "\n",
    "    home_bump = 0.015 if (pd.isna(g[\"weather_precip\"]) or g[\"weather_precip\"]==0) else 0.010\n",
    "    sign_home = 1 if home_is_fav else -1\n",
    "\n",
    "    base_market = p_fair_fav if not pd.isna(p_fair_fav) else p_spread\n",
    "    base_spread = p_spread if not pd.isna(p_spread) else p_fair_fav\n",
    "    if pd.isna(base_market) and pd.isna(base_spread):\n",
    "        p_win = np.nan\n",
    "    else:\n",
    "        p_market_adj = max(0,min(1,(base_market if not pd.isna(base_market) else base_spread) + sign_home*home_bump + (pen_dog - pen_fav)))\n",
    "        p_spread_adj  = max(0,min(1,(base_spread  if not pd.isna(base_spread)  else base_market) + sign_home*home_bump + (pen_dog - pen_fav)))\n",
    "        p_win = 0.5*(p_market_adj + p_spread_adj)\n",
    "\n",
    "    # EPA differentials\n",
    "    off_diff_fav = (g[\"off_epa_fav\"] - g[\"def_epa_allowed_dog\"]) if pd.notna(g[\"off_epa_fav\"]) and pd.notna(g[\"def_epa_allowed_dog\"]) else 0.0\n",
    "    off_diff_dog = (g[\"off_epa_dog\"] - g[\"def_epa_allowed_fav\"]) if pd.notna(g[\"off_epa_dog\"]) and pd.notna(g[\"def_epa_allowed_fav\"]) else 0.0\n",
    "\n",
    "    spread = float(g[\"spread_fav\"]) if pd.notna(g[\"spread_fav\"]) else 0.0\n",
    "    mu_margin = spread + 25*off_diff_fav - 20*off_diff_dog\n",
    "\n",
    "    # Monte Carlo cover\n",
    "    p_cover_mc, p_win_mc, pctiles = monte_carlo_cover(spread, mu_margin)\n",
    "\n",
    "    # Model cover preds: prefer real models if available\n",
    "    X1 = np.array([[spread, off_diff_fav, off_diff_dog]])\n",
    "    if 'logit_real' in globals() and logit_real is not None:\n",
    "        p_cover_logit = float(logit_real.predict_proba(X1)[:,1]); p_cover_gb = float(gb_real.predict_proba(X1)[:,1])\n",
    "        aucL, aucG = float(auc_logit_real), float(auc_gb_real)\n",
    "    else:\n",
    "        p_cover_logit = float(logit_s.predict_proba(X1)[:,1]);     p_cover_gb = float(gb_s.predict_proba(X1)[:,1])\n",
    "        aucL, aucG = float(aucL_s), float(aucG_s)\n",
    "\n",
    "    p_cover = float(np.mean([p_cover_mc, 0.5*(p_cover_logit + p_cover_gb)]))\n",
    "\n",
    "    ats_lean = ('Lean favorite cover' if p_cover>0.53 else 'Neutral / coin‑flip' if 0.47<=p_cover<=0.53 else 'Lean dog +points')\n",
    "\n",
    "    rows.append({\n",
    "        \"matchup\": f\"{g['away_team']} @ {g['home_team']}\",\n",
    "        \"favorite\": fav,\n",
    "        \"line\": f\"{fav} -{spread:.1f}\",\n",
    "        \"total\": g[\"total\"],\n",
    "        \"P(win fav)\": round(p_win,3) if not pd.isna(p_win) else np.nan,\n",
    "        \"P(cover fav)\": round(p_cover,3),\n",
    "        \"ATS lean\": ats_lean,\n",
    "        \"median margin\": round(pctiles[2],2),\n",
    "        \"p05\": round(pctiles[0],2),\n",
    "        \"p95\": round(pctiles[4],2),\n",
    "        \"vig_free_ml_win\": round(p_fair_fav,3) if not pd.isna(p_fair_fav) else np.nan,\n",
    "        \"spread_win_prob\": round(p_spread,3) if not pd.isna(p_spread) else np.nan,\n",
    "        \"home_bump_sign\": 1 if home_is_fav else -1,\n",
    "        \"inj_pen_fav\": round(pen_fav,3),\n",
    "        \"inj_pen_dog\": round(pen_dog,3),\n",
    "        \"off_diff_fav\": round(off_diff_fav,3),\n",
    "        \"off_diff_dog\": round(off_diff_dog,3),\n",
    "        \"AUC_logit\": round(aucL,3),\n",
    "        \"AUC_GBM\": round(aucG,3)\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).sort_values(\"matchup\").reset_index(drop=True)\n",
    "summary.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Wrote weekly summary to {OUTPUT_CSV}\")\n",
    "summary.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b401ef4",
   "metadata": {},
   "source": [
    "## 6) Snapshot interpretation from the current slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "summary = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "def snapshot(summary: pd.DataFrame):\n",
    "    # Highest win certainty — top 2 P(win fav)\n",
    "    top = summary[['matchup','favorite','line','P(win fav)']].dropna().sort_values('P(win fav)', ascending=False).head(2)\n",
    "    # Tightest games — P(win fav) closest to 0.5 (take 3)\n",
    "    tight = summary[['matchup','favorite','line','P(win fav)']].dropna().assign(diff=(summary['P(win fav)']-0.5).abs()).sort_values('diff').head(3)\n",
    "    # Most credible favorite covers — P(cover fav) >= 0.53\n",
    "    covers = summary.loc[summary['P(cover fav)']>=0.53, ['matchup','favorite','line','P(cover fav)']].sort_values('P(cover fav)', ascending=False)\n",
    "    # High variance no-plays — huge lines >= 13 and cover near coin flip\n",
    "    hv = summary.copy()\n",
    "    hv['abs_line'] = hv['line'].str.extract(r\"-([\\d\\.]+)\").astype(float)\n",
    "    hv = hv.loc[(hv['abs_line']>=13) & (hv['P(cover fav)'].between(0.48,0.52, inclusive='both')), ['matchup','favorite','line','P(cover fav)']]\n",
    "\n",
    "    print(\"Snapshot interpretation\")\n",
    "    if len(top):\n",
    "        print(\"  • Highest win certainty:\", \"; \".join([f\"{r.favorite} {r.line} in {r.matchup} (≈ {r['P(win fav)']:.0%})\" for _,r in top.iterrows()]))\n",
    "    if len(tight):\n",
    "        print(\"  • Tightest games:\", \"; \".join([f\"{r.matchup}\" for _,r in tight.iterrows()]))\n",
    "    if len(covers):\n",
    "        print(\"  • Most credible favorite covers:\", \"; \".join([f\\\"{r.favorite} {r.line}\\\" for _,r in covers.iterrows()]))\n",
    "    if len(hv):\n",
    "        print(\"  • High‑variance no‑plays:\", \"; \".join([f\\\"{r.favorite} {r.line}\\\" for _,r in hv.iterrows()]))\n",
    "\n",
    "snapshot(summary)\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3]",
   "language": "python",
   "name": "conda-env-miniforge3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
